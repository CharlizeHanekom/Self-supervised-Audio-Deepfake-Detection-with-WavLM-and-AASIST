{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913ce90f-450d-4c40-82cc-8a32e9858f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635fb6c8-b1ba-437c-ab9c-377cebcca350",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e412d86-9e7f-4803-97f7-63cb073f3627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchaudio in ./.local/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.12/site-packages (4.52.3)\n",
      "Requirement already satisfied: librosa in ./.local/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.12/site-packages (2.2.6)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: seaborn in ./.local/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.local/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.local/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.local/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.local/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.local/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.local/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.local/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.local/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.local/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.local/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.local/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.local/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.local/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.local/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.local/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.local/lib/python3.12/site-packages (from transformers) (0.32.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.local/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.local/lib/python3.12/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.local/lib/python3.12/site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.0 in ./.local/lib/python3.12/site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in ./.local/lib/python3.12/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./.local/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.local/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in ./.local/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.local/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.12/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.local/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./.local/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.local/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.local/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch torchaudio transformers librosa matplotlib numpy scikit-learn pandas seaborn tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a4308-211f-43f9-bb84-bc8b5093643b",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e11de5-e5a2-442e-8084-71a0ba0c523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta file:  ./data/InTheWild/meta.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths - adjust these to your JupyterHub environment\n",
    "BASE_PATH = \"./data/InTheWild\"  # Update this path\n",
    "DATASET_PATH = os.path.join(BASE_PATH, \"release_in_the_wild\")  # Update this path\n",
    "\n",
    "def load_InTheWild_metadata(base_path, dataset_path):\n",
    "    meta_file = os.path.join(base_path, \"meta.csv\")\n",
    "    print(\"Meta file: \", meta_file)\n",
    "    metadata = pd.read_csv(meta_file)\n",
    "    metadata['filepath'] = metadata['file'].apply(lambda x: os.path.join(dataset_path, x))\n",
    "    metadata['label'] = metadata['label'].apply(lambda x: 1 if x == 'spoof' else 0)\n",
    "    return metadata\n",
    "\n",
    "def load_ASVspoof_metadata(dataset_path):\n",
    "    protocol_file = os.path.join(dataset_path, 'protocol.txt')\n",
    "    metadata = pd.read_csv(protocol_file, sep=' ', header=None, \n",
    "                         names=['speaker_id', 'filename', 'unknown1', 'unknown2', 'label'])\n",
    "    metadata['filepath'] = metadata['filename'].apply(lambda x: os.path.join(dataset_path, 'flac', x + '.flac'))\n",
    "    metadata['label'] = metadata['label'].apply(lambda x: 1 if x == 'spoof' else 0)\n",
    "    return metadata\n",
    "\n",
    "# Load dataset\n",
    "inthewild_meta = load_InTheWild_metadata(BASE_PATH, DATASET_PATH)\n",
    "combined_meta = pd.concat([inthewild_meta], ignore_index=True)\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(combined_meta, test_size=0.2, random_state=42, stratify=combined_meta['label'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce040c2d-2d34-4e61-97d3-0d61c75e3567",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418d92b5-2211-4fed-b26e-b718e2fdc3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Initializing Training dataset\n",
      "==================================================\n",
      "Total samples: 22880\n",
      "Real/Fake ratio: 14373/8507\n",
      "\n",
      "==================================================\n",
      "Initializing Validation dataset\n",
      "==================================================\n",
      "Total samples: 2543\n",
      "Real/Fake ratio: 1597/946\n",
      "\n",
      "==================================================\n",
      "Initializing Test dataset\n",
      "==================================================\n",
      "Total samples: 6356\n",
      "Real/Fake ratio: 3993/2363\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata, sample_rate=16000, max_length=64600, name=\"dataset\"):\n",
    "        self.metadata = metadata\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = max_length\n",
    "        self.name = name\n",
    "        self._analyze_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(row['filepath'])\n",
    "\n",
    "            if sr != self.sample_rate:\n",
    "                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            if waveform.shape[1] < self.max_length:\n",
    "                pad_length = self.max_length - waveform.shape[1]\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, pad_length))\n",
    "            else:\n",
    "                waveform = waveform[:, :self.max_length]\n",
    "\n",
    "            return waveform.squeeze(0), torch.tensor(row['label'], dtype=torch.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError loading {row['filepath']}: {str(e)}\")\n",
    "            return torch.zeros(self.max_length), torch.tensor(-1, dtype=torch.float32)\n",
    "\n",
    "    def _analyze_dataset(self):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Initializing {self.name} dataset\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total samples: {len(self.metadata)}\")\n",
    "        print(f\"Real/Fake ratio: {sum(self.metadata['label']==0)}/{sum(self.metadata['label']==1)}\")\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = AudioDataset(train_df, name=\"Training\")\n",
    "val_dataset = AudioDataset(val_df, name=\"Validation\")\n",
    "test_dataset = AudioDataset(test_df, name=\"Test\")\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory = False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc9053-f39d-4979-960e-72eedc30cb05",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4aece46-91dc-455d-ac56-5bb76085ff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping models/models--microsoft--wavlm-base.zip to models...\n",
      "Unzip completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_dataset(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    Unzips the dataset file if it exists\n",
    "    \n",
    "    Parameters:\n",
    "    - zip_path (str): Path to the zip file\n",
    "    - extract_to (str): Directory to extract to\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if unzipped successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(zip_path):\n",
    "            print(f\"Zip file not found at {zip_path}\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"Unzipping {zip_path} to {extract_to}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(\"Unzip completed successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error unzipping file: {e}\")\n",
    "        return False\n",
    "\n",
    "zip_file_path = \"models/models--microsoft--wavlm-base.zip\"  # Change if your zip has different name\n",
    "\n",
    "if os.path.exists(zip_file_path):\n",
    "    unzip_success = unzip_dataset(zip_file_path, \"models\")\n",
    "else:\n",
    "    print(\"No zip file found, assuming dataset is already extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af428ab6-6d26-4fd8-81a0-c90442a59b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import WavLMModel, WavLMConfig\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class WavLMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_name: str = \"models/wavlm-base\", freeze: bool = True):\n",
    "        \"\"\"\n",
    "        WavLM feature extractor with optional fine-tuning\n",
    "        \n",
    "        Args:\n",
    "            model_name: Path to local pretrained WavLM model\n",
    "            freeze: Whether to freeze WavLM parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = WavLMConfig.from_pretrained(model_name)\n",
    "        self.wavlm = WavLMModel.from_pretrained(model_name)\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.wavlm.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.sample_rate = 16000  # WavLM's expected sample rate\n",
    "        self.output_dim = self.config.hidden_size\n",
    "\n",
    "    def forward(self, waveforms: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waveforms: Input audio tensor of shape (batch, seq_len) or (batch, 1, seq_len)\n",
    "        Returns:\n",
    "            features: Extracted features of shape (batch, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # Input validation and reshaping\n",
    "        if waveforms.dim() == 1:\n",
    "            waveforms = waveforms.unsqueeze(0)\n",
    "        elif waveforms.dim() == 3:\n",
    "            waveforms = waveforms.squeeze(1)\n",
    "            \n",
    "        # Normalize waveform to [-1, 1] if not already\n",
    "        if waveforms.abs().max() > 1:\n",
    "            waveforms = waveforms / (waveforms.abs().max() + 1e-8)\n",
    "            \n",
    "        outputs = self.wavlm(waveforms)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "class AASIST(nn.Module):\n",
    "    def __init__(self, input_dim: int = 768, num_heads: int = 4, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        AASIST model for audio spoofing detection\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input features\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Spectro-temporal processing\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=256,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features of shape (batch, seq_len, input_dim)\n",
    "        Returns:\n",
    "            predictions: Output scores of shape (batch,)\n",
    "        \"\"\"\n",
    "        # Conv1d expects (batch, channels, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_block(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.pool(x).squeeze(2)\n",
    "        \n",
    "        # Self-attention (expects seq_len, batch, channels)\n",
    "        x = x.unsqueeze(0)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + attn_output  # Residual connection\n",
    "        x = x.mean(dim=0)    # Average over sequence\n",
    "        \n",
    "        return self.classifier(x).squeeze(1)\n",
    "\n",
    "class WavLM_AASIST_Model(nn.Module):\n",
    "    def __init__(self, wavlm_model: str = \"microsoft/wavlm-base\", freeze_wavlm: bool = True):\n",
    "        \"\"\"\n",
    "        Combined WavLM + AASIST model for audio deepfake detection\n",
    "        \n",
    "        Args:\n",
    "            wavlm_model: Name of pretrained WavLM model\n",
    "            freeze_wavlm: Whether to freeze WavLM parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.feature_extractor = WavLMFeatureExtractor(wavlm_model, freeze_wavlm)\n",
    "        self.aasist = AASIST(input_dim=self.feature_extractor.output_dim)\n",
    "        \n",
    "    def forward(self, waveforms: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waveforms: Input audio tensor of shape (batch, seq_len) or (batch, 1, seq_len)\n",
    "        Returns:\n",
    "            predictions: Output scores of shape (batch,)\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor(waveforms)\n",
    "        return self.aasist(features)\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"Returns the dimension of the extracted features\"\"\"\n",
    "        return self.feature_extractor.output_dim\n",
    "\n",
    "# Ignore the tqdm warning (optional)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm.auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe984353-f9a7-452b-96da-944c92a63d86",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493a9fd-aeeb-4cdb-92d4-6884d3007fd1",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55dc3384-3dc6-4fc0-a684-5a96c3b34d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializeing model\n",
      "Criterion\n",
      "Optimizer\n",
      "Scheduler\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"Initializeing model\")\n",
    "model = WavLM_AASIST_Model(\n",
    "    wavlm_model=\"models/wavlm-base\",  # Point to your local model directory\n",
    "    freeze_wavlm=True\n",
    ").to(device)\n",
    "print(f\"Criterion\")\n",
    "criterion = nn.BCELoss()\n",
    "print(f\"Optimizer\")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "print(f\"Scheduler\")\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94b940-c7d2-4a59-9a70-7e7d2fd04b2c",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60aa1f9-876e-49a8-88ab-560fb450d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for 10 training steps: 78.04 sec\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/715 [00:42<8:27:54, 42.68s/it, loss=0.691, acc=0.531]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[1;32m     88\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(waveforms)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Use position=0 and leave=True for the main bar\n",
    "    with tqdm(dataloader, desc=\"Training\", position=0, leave=True) as pbar:\n",
    "        for i, (waveforms, labels) in enumerate(pbar):\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(waveforms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Update the progress bar description with current metrics\n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss/(i+1),\n",
    "                'acc': correct/total\n",
    "            })\n",
    "            \n",
    "    return running_loss / len(dataloader), correct / total\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use position=0 and leave=True for the main bar\n",
    "        with tqdm(dataloader, desc=\"Validating\", position=0, leave=True) as pbar:\n",
    "            for i, (waveforms, labels) in enumerate(pbar):\n",
    "                waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "                outputs = model(waveforms)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                preds = (outputs > 0.5).float()\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Update the progress bar description with current metrics\n",
    "                pbar.set_postfix({\n",
    "                    'loss': running_loss/(i+1),\n",
    "                    'acc': correct/total\n",
    "                })\n",
    "                \n",
    "    return running_loss / len(dataloader), correct / total\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "# start = time.time()\n",
    "# for i, (waveforms, labels) in enumerate(train_loader):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "# print(f\"Time to load 10 batches: {time.time() - start:.2f} sec\")\n",
    "\n",
    "waveforms, labels = next(iter(train_loader))\n",
    "waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(waveforms)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"Time for 10 training steps: {time.time() - start:.2f} sec\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    early_stopping(val_loss)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Acc: {train_acc:.2%} | Val Acc: {val_acc:.2%}\")\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79933ee-293e-49c4-82f9-aa8ad87b2c91",
   "metadata": {},
   "source": [
    "# Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877e3ae-6b50-43c9-8115-2cefe08b23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in dataloader:\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "            outputs = model(waveforms)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    preds = [1 if x > 0.5 else 0 for x in all_outputs]\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_outputs)\n",
    "    fnr = 1 - tpr\n",
    "    eer = fpr[np.nanargmin(np.absolute(fnr - fpr))]\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, preds),\n",
    "        'precision': precision_score(all_labels, preds),\n",
    "        'recall': recall_score(all_labels, preds),\n",
    "        'f1': f1_score(all_labels, preds),\n",
    "        'auc': roc_auc_score(all_labels, all_outputs),\n",
    "        'eer': eer\n",
    "    }\n",
    "\n",
    "# Evaluate\n",
    "test_metrics = evaluate_model(model, test_loader, device)\n",
    "print(\"\\nTest Metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train')\n",
    "plt.plot(val_accs, label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12cbcf-08bc-4472-b841-0815a2f68550",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841d6c2-dc48-475b-81b5-930567c37747",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'audio_deepfake_model.pth')\n",
    "\n",
    "# To load later:\n",
    "# model = WavLM_AASIST_Model().to(device)\n",
    "# model.load_state_dict(torch.load('audio_deepfake_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114b1ba-91d6-4d03-bc7d-b238dad8ccf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
