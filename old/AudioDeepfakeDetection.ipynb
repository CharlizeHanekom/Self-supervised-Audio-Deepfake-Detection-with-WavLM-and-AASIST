{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913ce90f-450d-4c40-82cc-8a32e9858f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635fb6c8-b1ba-437c-ab9c-377cebcca350",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26cb63-31a2-4c75-94f7-1c986e821d46",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch torchaudio transformers librosa matplotlib numpy scikit-learn pandas seaborn tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a4308-211f-43f9-bb84-bc8b5093643b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e11de5-e5a2-442e-8084-71a0ba0c523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta file:  ./data/InTheWild\\ITW_meta.csv\n",
      "Meta file:  ./data/ASVspoof\\ASV_meta.csv\n",
      "Meta file: ./data/FOR\\FOR_meta.csv\n",
      "✅ Final splits:\n",
      "Train: 22880 samples\n",
      "Val:   2543 samples\n",
      "Test:  6356 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "ITW_BASE_PATH = \"./data/InTheWild\"\n",
    "ASV_BASE_PATH = \"./data/ASVspoof\"\n",
    "FOR_BASE_PATH = \"./data/FOR\"\n",
    "\n",
    "ITW_DATASET_PATH = os.path.join(ITW_BASE_PATH, \"ITW_dataset\")\n",
    "ASV_DATASET_PATH = os.path.join(ASV_BASE_PATH, \"ASV_dataset\")\n",
    "FOR_DATASET_PATH = os.path.join(FOR_BASE_PATH, \"FOR_dataset\")\n",
    "\n",
    "def load_InTheWild_metadata(base_path, dataset_path):\n",
    "    meta_file = os.path.join(base_path, \"ITW_meta.csv\")\n",
    "    print(\"Meta file: \", meta_file)\n",
    "    metadata = pd.read_csv(meta_file)\n",
    "    metadata['filepath'] = metadata['file'].apply(lambda x: os.path.join(dataset_path, x))\n",
    "    metadata['label'] = metadata['label'].apply(lambda x: 1 if x == 'fake' else 0)\n",
    "    return metadata\n",
    "\n",
    "def load_ASVspoof_metadata(base_path, dataset_path):\n",
    "    meta_file = os.path.join(base_path, \"ASV_meta.csv\")\n",
    "    print(\"Meta file: \", meta_file)\n",
    "    metadata = pd.read_csv(meta_file)\n",
    "    metadata['filepath'] = metadata['file'].apply(lambda x: os.path.join(dataset_path, x))\n",
    "    metadata['label'] = metadata['label'].apply(lambda x: 1 if x == 'fake' else 0)\n",
    "    return metadata\n",
    "    \n",
    "def load_FOR_metadata_by_split(base_path, dataset_path):\n",
    "    meta_file = os.path.join(base_path, \"FOR_meta.csv\")\n",
    "    print(\"Meta file:\", meta_file)\n",
    "    df = pd.read_csv(meta_file)\n",
    "\n",
    "    df['filepath'] = df['file'].apply(lambda x: os.path.join(dataset_path, x))\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if x == 'fake' else 0)\n",
    "\n",
    "    # Make sure 'split' column is standardized\n",
    "    df['split'] = df['split'].str.lower().str.strip()\n",
    "\n",
    "    train_df = df[df['split'] == 'train'].copy()\n",
    "    val_df   = df[df['split'] == 'val'].copy()\n",
    "    test_df  = df[df['split'] == 'test'].copy()\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "inthewild_meta = load_InTheWild_metadata(ITW_BASE_PATH, ITW_DATASET_PATH)\n",
    "asv_meta = load_ASVspoof_metadata(ASV_BASE_PATH, ASV_DATASET_PATH)\n",
    "# inthewild_meta = load_InTheWild_metadata(BASE_PATH, DATASET_PATH)\n",
    "\n",
    "# Combine and split\n",
    "combined_meta = pd.concat([inthewild_meta], ignore_index=True) #[inthewild_meta, asv_meta]\n",
    "train_df, test_df = train_test_split(combined_meta, test_size=0.2, random_state=42, stratify=combined_meta['label'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "# Load FOR metadata by split\n",
    "for_train, for_val, for_test = load_FOR_metadata_by_split(FOR_BASE_PATH, FOR_DATASET_PATH)\n",
    "\n",
    "# Combine with other datasets\n",
    "# train_df = pd.concat([train_df, for_train], ignore_index=True)\n",
    "# val_df   = pd.concat([val_df, for_val], ignore_index=True)\n",
    "# test_df  = pd.concat([test_df, for_test], ignore_index=True)\n",
    "\n",
    "print(f\"✅ Final splits:\")\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Val:   {len(val_df)} samples\")\n",
    "print(f\"Test:  {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce040c2d-2d34-4e61-97d3-0d61c75e3567",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d92b5-2211-4fed-b26e-b718e2fdc3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Initializing Training dataset\n",
      "==================================================\n",
      "Total samples: 22880\n",
      "Real/Fake ratio: 14373/8507\n",
      "\n",
      "==================================================\n",
      "Initializing Validation dataset\n",
      "==================================================\n",
      "Total samples: 2543\n",
      "Real/Fake ratio: 1597/946\n",
      "\n",
      "==================================================\n",
      "Initializing Test dataset\n",
      "==================================================\n",
      "Total samples: 6356\n",
      "Real/Fake ratio: 3993/2363\n",
      "Using 0 DataLoader workers on 8 CPU cores.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata, sample_rate=16000, max_length=64600, name=\"dataset\"):\n",
    "        self.metadata = metadata\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = max_length\n",
    "        self.name = name\n",
    "        self._analyze_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(row['filepath'])\n",
    "\n",
    "            if sr != self.sample_rate:\n",
    "                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            if waveform.shape[1] < self.max_length:\n",
    "                pad_length = self.max_length - waveform.shape[1]\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, pad_length))\n",
    "            else:\n",
    "                waveform = waveform[:, :self.max_length]\n",
    "\n",
    "            return waveform.squeeze(0), torch.tensor(row['label'], dtype=torch.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError loading {row['filepath']}: {str(e)}\")\n",
    "            return torch.zeros(self.max_length), torch.tensor(-1, dtype=torch.float32)\n",
    "\n",
    "    def _analyze_dataset(self):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Initializing {self.name} dataset\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total samples: {len(self.metadata)}\")\n",
    "        print(f\"Real/Fake ratio: {sum(self.metadata['label']==0)}/{sum(self.metadata['label']==1)}\")\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = AudioDataset(train_df, name=\"Training\")\n",
    "val_dataset = AudioDataset(val_df, name=\"Validation\")\n",
    "test_dataset = AudioDataset(test_df, name=\"Test\")\n",
    "\n",
    "batch_size = 36\n",
    "num_workers = 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory = True, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory = True, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Skip batches if resuming mid-epoch\n",
    "if 'start_batch' in locals():\n",
    "    # Create iterator and skip batches\n",
    "    train_iterator = iter(train_loader)\n",
    "    for _ in range(start_batch):\n",
    "        next(train_iterator)\n",
    "else:\n",
    "    train_iterator = None\n",
    "    \n",
    "print(f\"Using {num_workers} DataLoader workers on {os.cpu_count()} CPU cores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc9053-f39d-4979-960e-72eedc30cb05",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4aece46-91dc-455d-ac56-5bb76085ff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No zip file found, assuming dataset is already extracted\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_dataset(zip_path, extract_to):\n",
    "    try:\n",
    "        if not os.path.exists(zip_path):\n",
    "            print(f\"Zip file not found at {zip_path}\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"Unzipping {zip_path} to {extract_to}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(\"Unzip completed successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error unzipping file: {e}\")\n",
    "        return False\n",
    "\n",
    "zip_file_path = \"models/models--microsoft--wavlm-base.zip\"  # Change if your zip has different name\n",
    "\n",
    "if os.path.exists(zip_file_path):\n",
    "    unzip_success = unzip_dataset(zip_file_path, \"models\")\n",
    "else:\n",
    "    print(\"No zip file found, assuming dataset is already extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af428ab6-6d26-4fd8-81a0-c90442a59b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import WavLMModel, WavLMConfig\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class WavLMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_name: str = \"models/wavlm-base\", freeze: bool = True):\n",
    "        \"\"\"\n",
    "        WavLM feature extractor with optional fine-tuning\n",
    "        \n",
    "        Args:\n",
    "            model_name: Path to local pretrained WavLM model\n",
    "            freeze: Whether to freeze WavLM parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = WavLMConfig.from_pretrained(model_name)\n",
    "        self.wavlm = WavLMModel.from_pretrained(model_name)\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.wavlm.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.sample_rate = 16000  # WavLM's expected sample rate\n",
    "        self.output_dim = self.config.hidden_size\n",
    "\n",
    "    def forward(self, waveforms: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waveforms: Input audio tensor of shape (batch, seq_len) or (batch, 1, seq_len)\n",
    "        Returns:\n",
    "            features: Extracted features of shape (batch, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # Input validation and reshaping\n",
    "        if waveforms.dim() == 1:\n",
    "            waveforms = waveforms.unsqueeze(0)\n",
    "        elif waveforms.dim() == 3:\n",
    "            waveforms = waveforms.squeeze(1)\n",
    "            \n",
    "        # Normalize waveform to [-1, 1] if not already\n",
    "        if waveforms.abs().max() > 1:\n",
    "            waveforms = waveforms / (waveforms.abs().max() + 1e-8)\n",
    "            \n",
    "        outputs = self.wavlm(waveforms)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "class AASIST(nn.Module):\n",
    "    def __init__(self, input_dim: int = 768, num_heads: int = 4, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        AASIST model for audio spoofing detection\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input features\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Spectro-temporal processing\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=256,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features of shape (batch, seq_len, input_dim)\n",
    "        Returns:\n",
    "            predictions: Output scores of shape (batch,)\n",
    "        \"\"\"\n",
    "        # Conv1d expects (batch, channels, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_block(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.pool(x).squeeze(2)\n",
    "        \n",
    "        # Self-attention (expects seq_len, batch, channels)\n",
    "        x = x.unsqueeze(0)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + attn_output  # Residual connection\n",
    "        x = x.mean(dim=0)    # Average over sequence\n",
    "        \n",
    "        return self.classifier(x).squeeze(1)\n",
    "\n",
    "class WavLM_AASIST_Model(nn.Module):\n",
    "    def __init__(self, wavlm_model: str = \"microsoft/wavlm-base\", freeze_wavlm: bool = True):\n",
    "        \"\"\"\n",
    "        Combined WavLM + AASIST model for audio deepfake detection\n",
    "        \n",
    "        Args:\n",
    "            wavlm_model: Name of pretrained WavLM model\n",
    "            freeze_wavlm: Whether to freeze WavLM parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.feature_extractor = WavLMFeatureExtractor(wavlm_model, freeze_wavlm)\n",
    "        self.aasist = AASIST(input_dim=self.feature_extractor.output_dim)\n",
    "        \n",
    "    def forward(self, waveforms: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waveforms: Input audio tensor of shape (batch, seq_len) or (batch, 1, seq_len)\n",
    "        Returns:\n",
    "            predictions: Output scores of shape (batch,)\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor(waveforms)\n",
    "        return self.aasist(features)\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"Returns the dimension of the extracted features\"\"\"\n",
    "        return self.feature_extractor.output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe984353-f9a7-452b-96da-944c92a63d86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493a9fd-aeeb-4cdb-92d4-6884d3007fd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44887e47-0c93-49bf-9373-b6b0b489dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== NEW: Checkpoint Setup =====\n",
    "checkpoint_frequency = 500  # Save checkpoint every 500 batches\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize training variables\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "# Resume if checkpoint exists\n",
    "last_checkpoint_path = f\"{CHECKPOINT_DIR}/last_checkpoint.pth\"\n",
    "if os.path.exists(last_checkpoint_path):\n",
    "    print(\"Resuming training from checkpoint...\")\n",
    "    checkpoint = torch.load(last_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Restore early stopping state\n",
    "    es_state = checkpoint['early_stopping_state']\n",
    "    early_stopping.counter = es_state['counter']\n",
    "    early_stopping.best_score = es_state['best_score']\n",
    "    early_stopping.early_stop = es_state['early_stop']\n",
    "    \n",
    "    # Restore training state\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    train_accs = checkpoint['train_accs']\n",
    "    val_accs = checkpoint['val_accs']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Resumed from epoch {start_epoch}\")\n",
    "# ===== END NEW ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc068be-b7ac-4ed6-9b38-1079f3aeaa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, batch_idx, model, optimizer, scheduler, early_stopping, train_losses, val_losses, train_accs, val_accs, best_val_loss):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'batch_idx': batch_idx,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'early_stopping_state': {\n",
    "            'counter': early_stopping.counter,\n",
    "            'best_score': early_stopping.best_score,\n",
    "            'early_stop': early_stopping.early_stop\n",
    "        },\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "    \n",
    "    filename = f\"checkpoint_epoch_{epoch}_batch_{batch_idx}.pth\"\n",
    "    torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, filename))\n",
    "    torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, \"last_checkpoint.pth\"))  # Always save last\n",
    "    print(f\"Saved checkpoint for epoch {epoch+1}, batch {batch_idx+1}\")\n",
    "\n",
    "if os.path.exists(last_checkpoint_path):\n",
    "    print(\"Resuming training from checkpoint...\")\n",
    "    checkpoint = torch.load(last_checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Restore training position\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_batch = checkpoint.get('batch_idx', 0) + 1  # Start from next batch\n",
    "    \n",
    "    # Load model and optimizer states\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Restore other states\n",
    "    es_state = checkpoint['early_stopping_state']\n",
    "    early_stopping.counter = es_state['counter']\n",
    "    early_stopping.best_score = es_state['best_score']\n",
    "    early_stopping.early_stop = es_state['early_stop']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    train_accs = checkpoint['train_accs']\n",
    "    val_accs = checkpoint['val_accs']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    \n",
    "    print(f\"Resumed from epoch {start_epoch+1}, batch {start_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc3384-3dc6-4fc0-a684-5a96c3b34d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializeing model\n",
      "Criterion\n",
      "Optimizer\n",
      "Scheduler\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"Initializeing model\")\n",
    "model = WavLM_AASIST_Model(\n",
    "    wavlm_model=\"models/wavlm-base\",  # Point to your local model directory\n",
    "    freeze_wavlm=True\n",
    ").to(device)\n",
    "print(f\"Criterion\")\n",
    "criterion = nn.BCELoss()\n",
    "print(f\"Optimizer\")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "print(f\"Scheduler\")\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94b940-c7d2-4a59-9a70-7e7d2fd04b2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60aa1f9-876e-49a8-88ab-560fb450d4fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loop\n",
      "\n",
      "waveforms next\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveforms to\n",
      "start tim\n",
      "for loop\n",
      "for loop\n",
      "for loop\n",
      "for loop\n",
      "for loop\n",
      "for loop\n",
      "for loop\n",
      "for loop\n",
      "for loop\n",
      "for loop\n",
      "Time for 10 training steps: 129.35 sec\n",
      "Train loop\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/636 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Redirect output to a log file ===\n",
    "# log_file = open(\"training_log.txt\", \"w\")\n",
    "# sys.stdout = log_file\n",
    "# sys.stderr = log_file\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Use position=0 and leave=True for the main bar\n",
    "    with tqdm(dataloader, desc=\"Training\", position=0, leave=True) as pbar:\n",
    "        for batch_idx, (waveforms, labels) in enumerate(pbar):\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(waveforms)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics calculation\n",
    "            running_loss += loss.item()\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Save intra-epoch checkpoint\n",
    "            if (batch_idx + 1) % checkpoint_frequency == 0:\n",
    "                save_checkpoint(\n",
    "                    epoch=epoch,\n",
    "                    batch_idx=batch_idx,\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    early_stopping=early_stopping,\n",
    "                    train_losses=train_losses,\n",
    "                    val_losses=val_losses,\n",
    "                    train_accs=train_accs,\n",
    "                    val_accs=val_accs,\n",
    "                    best_val_loss=best_val_loss\n",
    "                )\n",
    "                \n",
    "            # Update the progress bar description with current metrics\n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss/(batch_idx+1),\n",
    "                'acc': correct/total\n",
    "            })\n",
    "            \n",
    "            if epoch == start_epoch and batch_idx == 0:\n",
    "                start_batch = 0\n",
    "\n",
    "        # End of epoch processing\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "            \n",
    "    return running_loss / len(dataloader), correct / total\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use position=0 and leave=True for the main bar\n",
    "        with tqdm(dataloader, desc=\"Validating\", position=0, leave=True, file=sys.stdout) as pbar:\n",
    "            for i, (waveforms, labels) in enumerate(pbar):\n",
    "                waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "                outputs = model(waveforms)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                preds = (outputs > 0.5).float()\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Update the progress bar description with current metrics\n",
    "                pbar.set_postfix({\n",
    "                    'loss': running_loss/(i+1),\n",
    "                    'acc': correct/total\n",
    "                })\n",
    "                \n",
    "    return running_loss / len(dataloader), correct / total\n",
    "\n",
    "# Training loop\n",
    "print(\"Training Loop\\n\")\n",
    "num_epochs = 20\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "print(\"waveforms next\")\n",
    "waveforms, labels = next(iter(train_loader))\n",
    "print(\"waveforms to\")\n",
    "waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "\n",
    "print(\"start tim\")\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    print(\"for loop\")\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(waveforms)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"Time for 10 training steps: {time.time() - start:.2f} sec\")\n",
    "\n",
    "# Modified training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(\"Train loop\")\n",
    "    loader = train_iterator if train_iterator else train_loader\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Append metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Update best validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    \n",
    "    # ===== NEW: Save checkpoint =====\n",
    "    save_checkpoint(epoch)\n",
    "    # ===== END NEW =====\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    early_stopping(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Acc: {train_acc:.2%} | Val Acc: {val_acc:.2%}\")\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "    \n",
    "    train_iterator = None\n",
    "    \n",
    "# log_file.close()\n",
    "# sys.stdout = sys.__stdout__\n",
    "# sys.stderr = sys.__stderr__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841d6c2-dc48-475b-81b5-930567c37747",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(model.state_dict(), 'audio_deepfake_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79933ee-293e-49c4-82f9-aa8ad87b2c91",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0ae78-4099-4d04-a064-5d7bbf0684cd",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f20f1-6296-4c7f-9bd4-97b05354275c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load saved model\n",
    "# print(\"\\nLoading model\")\n",
    "# model = WavLM_AASIST_Model(\n",
    "#     wavlm_model=\"models/wavlm-base\",  # Point to your local model directory\n",
    "#     freeze_wavlm=True).to(device)\n",
    "# model.load_state_dict(torch.load('audio_deepfake_model.pth'))\n",
    "# print(\"\\nModel successfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877e3ae-6b50-43c9-8115-2cefe08b23a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in dataloader:\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "            outputs = model(waveforms)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    preds = [1 if x > 0.5 else 0 for x in all_outputs]\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_outputs)\n",
    "    fnr = 1 - tpr\n",
    "    eer = fpr[np.nanargmin(np.absolute(fnr - fpr))]\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, preds),\n",
    "        'precision': precision_score(all_labels, preds),\n",
    "        'recall': recall_score(all_labels, preds),\n",
    "        'f1': f1_score(all_labels, preds),\n",
    "        'auc': roc_auc_score(all_labels, all_outputs),\n",
    "        'eer': eer\n",
    "    }\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating\")\n",
    "test_metrics = evaluate_model(model, test_loader, device)\n",
    "print(\"\\nTest Metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train')\n",
    "plt.plot(val_accs, label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c62b2-aae9-4f31-9168-74905ef51414",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f9e59-0e16-487b-88d1-156ab8edc65a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "            outputs = model(waveforms)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Real', 'Fake'], \n",
    "                yticklabels=['Real', 'Fake'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Plot confusion matrices for train, validation and test sets\n",
    "print(\"\\nTraining Set Confusion Matrix:\")\n",
    "train_cm = plot_confusion_matrix(model, train_loader, device)\n",
    "\n",
    "print(\"\\nValidation Set Confusion Matrix:\")\n",
    "val_cm = plot_confusion_matrix(model, val_loader, device)\n",
    "\n",
    "print(\"\\nTest Set Confusion Matrix:\")\n",
    "test_cm = plot_confusion_matrix(model, test_loader, device)\n",
    "\n",
    "# Print detailed metrics from confusion matrix\n",
    "def print_metrics_from_cm(cm, set_name):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{set_name} Metrics from Confusion Matrix:\")\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print_metrics_from_cm(train_cm, \"Training\")\n",
    "print_metrics_from_cm(val_cm, \"Validation\")\n",
    "print_metrics_from_cm(test_cm, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12cbcf-08bc-4472-b841-0815a2f68550",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379523b-f576-44eb-aeb9-0dc69fccecfa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'audio_deepfake_model.pth')\n",
    "\n",
    "# To load later:\n",
    "# model = WavLM_AASIST_Model().to(device)\n",
    "# model.load_state_dict(torch.load('audio_deepfake_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356ac05-eba1-44fe-8d65-127c4bb53416",
   "metadata": {},
   "source": [
    "# For later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ede11-90c3-46bb-89cd-d672c2a67baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. First run all the cell with model class definitions\n",
    "# (WavLMFeatureExtractor, AASIST, WavLM_AASIST_Model)\n",
    "\n",
    "# 2. Then run:\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 3. Initialize and load model\n",
    "model = WavLM_AASIST_Model().to(device)\n",
    "model.load_state_dict(torch.load('audio_deepfake_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 4. Now you can use it for predictions\n",
    "def predict_audio(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    # Add any preprocessing you used during training\n",
    "    waveform = waveform.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(waveform)\n",
    "    return \"Fake\" if output > 0.5 else \"Real\", float(output)\n",
    "\n",
    "# Example usage:\n",
    "# prediction, confidence = predict_audio(\"test_audio.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
